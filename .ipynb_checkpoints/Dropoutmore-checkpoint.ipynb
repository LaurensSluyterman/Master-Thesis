{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import losses\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "import numpy\n",
    "from keras.layers import Dense, Flatten, Activation, Dropout\n",
    "from keras.utils import normalize, to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setting up tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"my_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_run_logdir(): \n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\") \n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir() \n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Messing around with tfp stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Normal 'Normal/' batch_shape=[] event_shape=[] dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = tfd.Normal(loc=0., scale=1.)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6108214"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.sample().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.linspace(-10,10,1000)\n",
    "y=np.exp(n.log_prob(x).numpy())\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.Bernoulli 'Bernoulli/' batch_shape=[] event_shape=[] dtype=int32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tfd.Bernoulli(probs=0.7)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=341, shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 21:40:54.943908 4465960384 deprecation.py:323] From /anaconda3/envs/Tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:182: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(b.log_prob(1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfp.distributions.MultivariateNormalDiag 'MultivariateNormalDiag/' batch_shape=[] event_shape=[2] dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd = tfd.MultivariateNormalDiag(loc=[0., 10.], scale_diag=[1., 4.])\n",
    "nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2644083, 10.177029 ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.sample().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPG example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code in the first part of this example comes from: https://www.tensorflow.org/tutorials/keras/basic_regression. I used the methods developed by googles tensorflow probability team and adapted some of their example code. The definitions of the priors and posteriors is not my own work. The gaussian process part is also largely copied from their example codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download and preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly copied from the example code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download and examine a standard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/laurens/.keras/datasets/auto-mpg.data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
       "393  27.0          4         140.0        86.0  2790.0          15.6   \n",
       "394  44.0          4          97.0        52.0  2130.0          24.6   \n",
       "395  32.0          4         135.0        84.0  2295.0          11.6   \n",
       "396  28.0          4         120.0        79.0  2625.0          18.6   \n",
       "397  31.0          4         119.0        82.0  2720.0          19.4   \n",
       "\n",
       "     Model Year  Origin  \n",
       "393          82       1  \n",
       "394          82       2  \n",
       "395          82       1  \n",
       "396          82       1  \n",
       "397          82       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
    "                      na_values = \"?\", comment='\\t',\n",
    "                      sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = dataset.pop('Origin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>USA</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Japan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
       "393  27.0          4         140.0        86.0  2790.0          15.6   \n",
       "394  44.0          4          97.0        52.0  2130.0          24.6   \n",
       "395  32.0          4         135.0        84.0  2295.0          11.6   \n",
       "396  28.0          4         120.0        79.0  2625.0          18.6   \n",
       "397  31.0          4         119.0        82.0  2720.0          19.4   \n",
       "\n",
       "     Model Year  USA  Europe  Japan  \n",
       "393          82  1.0     0.0    0.0  \n",
       "394          82  0.0     1.0    0.0  \n",
       "395          82  1.0     0.0    0.0  \n",
       "396          82  1.0     0.0    0.0  \n",
       "397          82  1.0     0.0    0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['USA'] = (origin == 1)*1.0\n",
    "dataset['Europe'] = (origin == 2)*1.0\n",
    "dataset['Japan'] = (origin == 3)*1.0\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cylinders</th>\n",
       "      <td>314.0</td>\n",
       "      <td>5.477707</td>\n",
       "      <td>1.699788</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Displacement</th>\n",
       "      <td>314.0</td>\n",
       "      <td>195.318471</td>\n",
       "      <td>104.331589</td>\n",
       "      <td>68.0</td>\n",
       "      <td>105.50</td>\n",
       "      <td>151.0</td>\n",
       "      <td>265.75</td>\n",
       "      <td>455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horsepower</th>\n",
       "      <td>314.0</td>\n",
       "      <td>104.869427</td>\n",
       "      <td>38.096214</td>\n",
       "      <td>46.0</td>\n",
       "      <td>76.25</td>\n",
       "      <td>94.5</td>\n",
       "      <td>128.00</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight</th>\n",
       "      <td>314.0</td>\n",
       "      <td>2990.251592</td>\n",
       "      <td>843.898596</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>2256.50</td>\n",
       "      <td>2822.5</td>\n",
       "      <td>3608.00</td>\n",
       "      <td>5140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acceleration</th>\n",
       "      <td>314.0</td>\n",
       "      <td>15.559236</td>\n",
       "      <td>2.789230</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.80</td>\n",
       "      <td>15.5</td>\n",
       "      <td>17.20</td>\n",
       "      <td>24.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model Year</th>\n",
       "      <td>314.0</td>\n",
       "      <td>75.898089</td>\n",
       "      <td>3.675642</td>\n",
       "      <td>70.0</td>\n",
       "      <td>73.00</td>\n",
       "      <td>76.0</td>\n",
       "      <td>79.00</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USA</th>\n",
       "      <td>314.0</td>\n",
       "      <td>0.624204</td>\n",
       "      <td>0.485101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Europe</th>\n",
       "      <td>314.0</td>\n",
       "      <td>0.178344</td>\n",
       "      <td>0.383413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japan</th>\n",
       "      <td>314.0</td>\n",
       "      <td>0.197452</td>\n",
       "      <td>0.398712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count         mean         std     min      25%     50%  \\\n",
       "Cylinders     314.0     5.477707    1.699788     3.0     4.00     4.0   \n",
       "Displacement  314.0   195.318471  104.331589    68.0   105.50   151.0   \n",
       "Horsepower    314.0   104.869427   38.096214    46.0    76.25    94.5   \n",
       "Weight        314.0  2990.251592  843.898596  1649.0  2256.50  2822.5   \n",
       "Acceleration  314.0    15.559236    2.789230     8.0    13.80    15.5   \n",
       "Model Year    314.0    75.898089    3.675642    70.0    73.00    76.0   \n",
       "USA           314.0     0.624204    0.485101     0.0     0.00     1.0   \n",
       "Europe        314.0     0.178344    0.383413     0.0     0.00     0.0   \n",
       "Japan         314.0     0.197452    0.398712     0.0     0.00     0.0   \n",
       "\n",
       "                  75%     max  \n",
       "Cylinders        8.00     8.0  \n",
       "Displacement   265.75   455.0  \n",
       "Horsepower     128.00   225.0  \n",
       "Weight        3608.00  5140.0  \n",
       "Acceleration    17.20    24.8  \n",
       "Model Year      79.00    82.0  \n",
       "USA              1.00     1.0  \n",
       "Europe           0.00     1.0  \n",
       "Japan            0.00     1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"MPG\")\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_dataset.pop('MPG')\n",
    "test_labels = test_dataset.pop('MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "  return (x - train_stats['mean']) / train_stats['std']\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn=50\n",
    "example_batch = normed_test_data[:tn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builing a dropout model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "negloglik = lambda y, rv_y: -rv_y.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): #probabilistic\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    layers.Dense(64, activation=tf.nn.relu), \n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dense(2,name='meanvar'),\n",
    "    tfp.layers.DistributionLambda(\n",
    "      lambda t: tfd.Normal(loc=t[..., :1],\n",
    "                           scale=1e-3 + tf.math.softplus(0.05 * t[...,1:]))),\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss=negloglik,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_squared_error'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 9)]               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                640       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "elu_1 (ELU)                  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "distribution_lambda_1 (Distr ((None, 1), (None, 1))    0         \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=len(train_dataset.keys()))\n",
    "d0=layers.Dropout(0.1)(inputs,training=True)\n",
    "c1= layers.Dense(64)(d0)\n",
    "d1=layers.Dropout(0.5)(c1,training=True)\n",
    "a1=tf.keras.layers.ELU(alpha=1.0)(d1)\n",
    "c2=layers.Dense(2)(a1)\n",
    "#d2=layers.Dropout(0.00)(c2,training=True)\n",
    "#a2=tf.keras.layers.ELU(alpha=1.0)(d2)\n",
    "o=tfp.layers.DistributionLambda(\n",
    "      lambda t: tfd.Normal(loc=t[..., :1],\n",
    "                           scale=1e-3 + tf.math.softplus(0.05 * t[...,1:])))(c2) #The scale is stdev!!\n",
    "#outputs=layers.Activation('softmax')(c2)\n",
    "modeld2= tf.keras.Model(inputs, o)\n",
    "modeld2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeld2.compile(\n",
    "    loss=negloglik,\n",
    "    optimizer='adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 251 samples, validate on 63 samples\n",
      "Epoch 1/200\n",
      "251/251 [==============================] - 0s 49us/sample - loss: 8.0376 - val_loss: 7.9863\n",
      "Epoch 2/200\n",
      "251/251 [==============================] - 0s 44us/sample - loss: 7.4580 - val_loss: 12.2954\n",
      "Epoch 3/200\n",
      "251/251 [==============================] - 0s 64us/sample - loss: 8.3727 - val_loss: 10.1584\n",
      "Epoch 4/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 7.5100 - val_loss: 7.5606\n",
      "Epoch 5/200\n",
      "251/251 [==============================] - 0s 41us/sample - loss: 7.9367 - val_loss: 9.3223\n",
      "Epoch 6/200\n",
      "251/251 [==============================] - 0s 81us/sample - loss: 7.6511 - val_loss: 11.5807\n",
      "Epoch 7/200\n",
      "251/251 [==============================] - 0s 65us/sample - loss: 8.1599 - val_loss: 7.2345\n",
      "Epoch 8/200\n",
      "251/251 [==============================] - 0s 63us/sample - loss: 7.9410 - val_loss: 8.7682\n",
      "Epoch 9/200\n",
      "251/251 [==============================] - 0s 70us/sample - loss: 7.7854 - val_loss: 6.3082\n",
      "Epoch 10/200\n",
      "251/251 [==============================] - 0s 60us/sample - loss: 7.2579 - val_loss: 6.9084\n",
      "Epoch 11/200\n",
      "251/251 [==============================] - 0s 87us/sample - loss: 7.8002 - val_loss: 6.0109\n",
      "Epoch 12/200\n",
      "251/251 [==============================] - 0s 70us/sample - loss: 8.5997 - val_loss: 10.3518\n",
      "Epoch 13/200\n",
      "251/251 [==============================] - 0s 65us/sample - loss: 7.7696 - val_loss: 10.4840\n",
      "Epoch 14/200\n",
      "251/251 [==============================] - 0s 48us/sample - loss: 7.5167 - val_loss: 7.3671\n",
      "Epoch 15/200\n",
      "251/251 [==============================] - 0s 44us/sample - loss: 8.2784 - val_loss: 8.0845\n",
      "Epoch 16/200\n",
      "251/251 [==============================] - 0s 48us/sample - loss: 6.9994 - val_loss: 10.1342\n",
      "Epoch 17/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 7.3862 - val_loss: 6.5308\n",
      "Epoch 18/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 6.8591 - val_loss: 8.2173\n",
      "Epoch 19/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 7.1712 - val_loss: 9.3260\n",
      "Epoch 20/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 6.8118 - val_loss: 7.7869\n",
      "Epoch 21/200\n",
      "251/251 [==============================] - 0s 44us/sample - loss: 7.9664 - val_loss: 8.6018\n",
      "Epoch 22/200\n",
      "251/251 [==============================] - 0s 42us/sample - loss: 6.3969 - val_loss: 9.2373\n",
      "Epoch 23/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 8.2667 - val_loss: 7.0794\n",
      "Epoch 24/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 7.8765 - val_loss: 7.2574\n",
      "Epoch 25/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 6.8292 - val_loss: 6.6075\n",
      "Epoch 26/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 7.0261 - val_loss: 7.1292\n",
      "Epoch 27/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 6.9562 - val_loss: 7.6360\n",
      "Epoch 28/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 6.8924 - val_loss: 6.1246\n",
      "Epoch 29/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 6.4762 - val_loss: 7.8304\n",
      "Epoch 30/200\n",
      "251/251 [==============================] - 0s 29us/sample - loss: 5.9166 - val_loss: 6.5029\n",
      "Epoch 31/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 6.3577 - val_loss: 7.1797\n",
      "Epoch 32/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 7.2759 - val_loss: 7.9902\n",
      "Epoch 33/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 7.8466 - val_loss: 7.0925\n",
      "Epoch 34/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 7.1779 - val_loss: 9.5075\n",
      "Epoch 35/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 7.0095 - val_loss: 7.6582\n",
      "Epoch 36/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 6.8764 - val_loss: 7.0339\n",
      "Epoch 37/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 6.4390 - val_loss: 7.4902\n",
      "Epoch 38/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 8.0053 - val_loss: 7.6138\n",
      "Epoch 39/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 6.7574 - val_loss: 8.7422\n",
      "Epoch 40/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 6.2281 - val_loss: 6.7090\n",
      "Epoch 41/200\n",
      "251/251 [==============================] - 0s 42us/sample - loss: 7.7767 - val_loss: 7.0506\n",
      "Epoch 42/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 6.0730 - val_loss: 6.1492\n",
      "Epoch 43/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 6.5672 - val_loss: 6.7039\n",
      "Epoch 44/200\n",
      "251/251 [==============================] - 0s 42us/sample - loss: 6.1424 - val_loss: 6.9044\n",
      "Epoch 45/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 7.4133 - val_loss: 7.4634\n",
      "Epoch 46/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 6.7594 - val_loss: 6.9529\n",
      "Epoch 47/200\n",
      "251/251 [==============================] - 0s 41us/sample - loss: 6.6695 - val_loss: 6.0611\n",
      "Epoch 48/200\n",
      "251/251 [==============================] - 0s 43us/sample - loss: 6.4411 - val_loss: 5.1211\n",
      "Epoch 49/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 6.1361 - val_loss: 8.6445\n",
      "Epoch 50/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 7.4236 - val_loss: 8.2073\n",
      "Epoch 51/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 6.2510 - val_loss: 6.5216\n",
      "Epoch 52/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 7.5527 - val_loss: 5.8542\n",
      "Epoch 53/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 7.1686 - val_loss: 8.2330\n",
      "Epoch 54/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 5.8704 - val_loss: 7.6282\n",
      "Epoch 55/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 7.1790 - val_loss: 5.3555\n",
      "Epoch 56/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 6.7213 - val_loss: 7.1418\n",
      "Epoch 57/200\n",
      "251/251 [==============================] - 0s 42us/sample - loss: 5.6102 - val_loss: 6.9765\n",
      "Epoch 58/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 6.3917 - val_loss: 6.2583\n",
      "Epoch 59/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 6.9911 - val_loss: 5.2461\n",
      "Epoch 60/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 6.6852 - val_loss: 5.7801\n",
      "Epoch 61/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.7366 - val_loss: 5.5659\n",
      "Epoch 62/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 6.5438 - val_loss: 7.1042\n",
      "Epoch 63/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 7.1231 - val_loss: 6.4523\n",
      "Epoch 64/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 6.2927 - val_loss: 9.0433\n",
      "Epoch 65/200\n",
      "251/251 [==============================] - 0s 42us/sample - loss: 6.0543 - val_loss: 5.8819\n",
      "Epoch 66/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.6378 - val_loss: 6.3761\n",
      "Epoch 67/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 6.8892 - val_loss: 5.4341\n",
      "Epoch 68/200\n",
      "251/251 [==============================] - 0s 43us/sample - loss: 6.3175 - val_loss: 5.1599\n",
      "Epoch 69/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.7800 - val_loss: 4.8560\n",
      "Epoch 70/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 6.1021 - val_loss: 4.6837\n",
      "Epoch 71/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 6.3035 - val_loss: 6.4153\n",
      "Epoch 72/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 5.2096 - val_loss: 5.1765\n",
      "Epoch 73/200\n",
      "251/251 [==============================] - 0s 41us/sample - loss: 5.2702 - val_loss: 7.9617\n",
      "Epoch 74/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 6.7474 - val_loss: 8.3631\n",
      "Epoch 75/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 7.9156 - val_loss: 6.8690\n",
      "Epoch 76/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 5.5420 - val_loss: 5.3080\n",
      "Epoch 77/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 7.2933 - val_loss: 7.3940\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 0s 40us/sample - loss: 5.3035 - val_loss: 7.8192\n",
      "Epoch 79/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 6.3958 - val_loss: 6.5337\n",
      "Epoch 80/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.5653 - val_loss: 5.8139\n",
      "Epoch 81/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 5.6453 - val_loss: 5.2125\n",
      "Epoch 82/200\n",
      "251/251 [==============================] - 0s 28us/sample - loss: 5.8318 - val_loss: 7.5584\n",
      "Epoch 83/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 6.3455 - val_loss: 4.6507\n",
      "Epoch 84/200\n",
      "251/251 [==============================] - 0s 30us/sample - loss: 5.9787 - val_loss: 5.0856\n",
      "Epoch 85/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.9686 - val_loss: 5.1404\n",
      "Epoch 86/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 5.7202 - val_loss: 6.1657\n",
      "Epoch 87/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 6.1405 - val_loss: 5.3989\n",
      "Epoch 88/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 5.0894 - val_loss: 5.2295\n",
      "Epoch 89/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 5.5221 - val_loss: 7.4349\n",
      "Epoch 90/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 6.6014 - val_loss: 5.8740\n",
      "Epoch 91/200\n",
      "251/251 [==============================] - 0s 30us/sample - loss: 6.0015 - val_loss: 5.7709\n",
      "Epoch 92/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 6.0886 - val_loss: 5.6820\n",
      "Epoch 93/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 5.7730 - val_loss: 6.1477\n",
      "Epoch 94/200\n",
      "251/251 [==============================] - 0s 42us/sample - loss: 6.0036 - val_loss: 7.9736\n",
      "Epoch 95/200\n",
      "251/251 [==============================] - 0s 30us/sample - loss: 6.1496 - val_loss: 5.2148\n",
      "Epoch 96/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 5.6519 - val_loss: 5.5370\n",
      "Epoch 97/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.2897 - val_loss: 6.8280\n",
      "Epoch 98/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 6.3733 - val_loss: 7.7437\n",
      "Epoch 99/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 6.3021 - val_loss: 7.6667\n",
      "Epoch 100/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 6.1721 - val_loss: 5.8257\n",
      "Epoch 101/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 4.5638 - val_loss: 6.3607\n",
      "Epoch 102/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.9965 - val_loss: 6.1999\n",
      "Epoch 103/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 6.5737 - val_loss: 6.2135\n",
      "Epoch 104/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 5.3865 - val_loss: 4.9120\n",
      "Epoch 105/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 5.4039 - val_loss: 5.5387\n",
      "Epoch 106/200\n",
      "251/251 [==============================] - 0s 41us/sample - loss: 4.9693 - val_loss: 6.4680\n",
      "Epoch 107/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 5.9502 - val_loss: 5.4536\n",
      "Epoch 108/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 6.5732 - val_loss: 5.6663\n",
      "Epoch 109/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.9924 - val_loss: 6.1469\n",
      "Epoch 110/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 5.4848 - val_loss: 5.9960\n",
      "Epoch 111/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 5.6528 - val_loss: 4.8938\n",
      "Epoch 112/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 4.8480 - val_loss: 8.5263\n",
      "Epoch 113/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 4.9086 - val_loss: 5.7778\n",
      "Epoch 114/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 6.1046 - val_loss: 6.7510\n",
      "Epoch 115/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 5.7001 - val_loss: 4.8228\n",
      "Epoch 116/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.6145 - val_loss: 6.5171\n",
      "Epoch 117/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 5.1094 - val_loss: 5.3048\n",
      "Epoch 118/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 5.2973 - val_loss: 5.3358\n",
      "Epoch 119/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 5.2805 - val_loss: 5.3740\n",
      "Epoch 120/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 4.6050 - val_loss: 6.9599\n",
      "Epoch 121/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 5.2367 - val_loss: 5.6070\n",
      "Epoch 122/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.1685 - val_loss: 4.3415\n",
      "Epoch 123/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 5.4184 - val_loss: 5.0540\n",
      "Epoch 124/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.3531 - val_loss: 4.6951\n",
      "Epoch 125/200\n",
      "251/251 [==============================] - 0s 41us/sample - loss: 5.3515 - val_loss: 4.5962\n",
      "Epoch 126/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 4.9063 - val_loss: 4.9395\n",
      "Epoch 127/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.6613 - val_loss: 4.1132\n",
      "Epoch 128/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.2616 - val_loss: 5.0493\n",
      "Epoch 129/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 5.0162 - val_loss: 5.2254\n",
      "Epoch 130/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.1293 - val_loss: 5.0987\n",
      "Epoch 131/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 6.0224 - val_loss: 7.2499\n",
      "Epoch 132/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 4.5964 - val_loss: 5.9075\n",
      "Epoch 133/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 4.7578 - val_loss: 4.6059\n",
      "Epoch 134/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 5.0872 - val_loss: 4.7299\n",
      "Epoch 135/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.0045 - val_loss: 5.8720\n",
      "Epoch 136/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.7945 - val_loss: 5.6848\n",
      "Epoch 137/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 4.3251 - val_loss: 4.8811\n",
      "Epoch 138/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.7496 - val_loss: 6.1576\n",
      "Epoch 139/200\n",
      "251/251 [==============================] - 0s 42us/sample - loss: 4.9991 - val_loss: 6.1221\n",
      "Epoch 140/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 4.6178 - val_loss: 7.0919\n",
      "Epoch 141/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 5.4898 - val_loss: 5.0445\n",
      "Epoch 142/200\n",
      "251/251 [==============================] - 0s 45us/sample - loss: 4.8587 - val_loss: 4.4456\n",
      "Epoch 143/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 5.1433 - val_loss: 4.7337\n",
      "Epoch 144/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.2501 - val_loss: 5.8197\n",
      "Epoch 145/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 4.9484 - val_loss: 7.0185\n",
      "Epoch 146/200\n",
      "251/251 [==============================] - 0s 30us/sample - loss: 4.9778 - val_loss: 7.0700\n",
      "Epoch 147/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 5.0409 - val_loss: 6.5328\n",
      "Epoch 148/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 4.9095 - val_loss: 4.5729\n",
      "Epoch 149/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 4.9850 - val_loss: 4.9341\n",
      "Epoch 150/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 4.9917 - val_loss: 5.1606\n",
      "Epoch 151/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 4.7544 - val_loss: 4.6319\n",
      "Epoch 152/200\n",
      "251/251 [==============================] - 0s 42us/sample - loss: 4.8503 - val_loss: 5.7884\n",
      "Epoch 153/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 5.2935 - val_loss: 5.7758\n",
      "Epoch 154/200\n",
      "251/251 [==============================] - 0s 41us/sample - loss: 4.7803 - val_loss: 4.2125\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 0s 36us/sample - loss: 5.2459 - val_loss: 5.3104\n",
      "Epoch 156/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 4.8913 - val_loss: 5.9569\n",
      "Epoch 157/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 4.6908 - val_loss: 5.9179\n",
      "Epoch 158/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 4.0304 - val_loss: 4.2141\n",
      "Epoch 159/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 5.2501 - val_loss: 5.1585\n",
      "Epoch 160/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 4.5752 - val_loss: 7.2076\n",
      "Epoch 161/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.1050 - val_loss: 4.4972\n",
      "Epoch 162/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 5.5728 - val_loss: 3.6340\n",
      "Epoch 163/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 4.6387 - val_loss: 6.2378\n",
      "Epoch 164/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 5.1307 - val_loss: 5.8347\n",
      "Epoch 165/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 4.9805 - val_loss: 4.2953\n",
      "Epoch 166/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 4.7643 - val_loss: 6.4645\n",
      "Epoch 167/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 5.0757 - val_loss: 4.9384\n",
      "Epoch 168/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 4.9290 - val_loss: 5.1857\n",
      "Epoch 169/200\n",
      "251/251 [==============================] - 0s 39us/sample - loss: 4.5226 - val_loss: 4.4559\n",
      "Epoch 170/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 4.6114 - val_loss: 4.5188\n",
      "Epoch 171/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 4.9543 - val_loss: 4.8020\n",
      "Epoch 172/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 5.3070 - val_loss: 4.0540\n",
      "Epoch 173/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 4.6606 - val_loss: 5.4579\n",
      "Epoch 174/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 5.2284 - val_loss: 3.8786\n",
      "Epoch 175/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 4.4716 - val_loss: 5.0221\n",
      "Epoch 176/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 4.6484 - val_loss: 4.6529\n",
      "Epoch 177/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 4.8764 - val_loss: 4.9309\n",
      "Epoch 178/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 4.0617 - val_loss: 5.5788\n",
      "Epoch 179/200\n",
      "251/251 [==============================] - 0s 30us/sample - loss: 4.9736 - val_loss: 4.1521\n",
      "Epoch 180/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 5.2368 - val_loss: 5.2470\n",
      "Epoch 181/200\n",
      "251/251 [==============================] - 0s 29us/sample - loss: 4.1419 - val_loss: 4.2098\n",
      "Epoch 182/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 4.6461 - val_loss: 4.6269\n",
      "Epoch 183/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 4.5317 - val_loss: 5.1129\n",
      "Epoch 184/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 4.8177 - val_loss: 4.9410\n",
      "Epoch 185/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 5.1549 - val_loss: 4.4346\n",
      "Epoch 186/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 4.5565 - val_loss: 4.7135\n",
      "Epoch 187/200\n",
      "251/251 [==============================] - 0s 33us/sample - loss: 4.9136 - val_loss: 4.5990\n",
      "Epoch 188/200\n",
      "251/251 [==============================] - 0s 34us/sample - loss: 4.6064 - val_loss: 4.9310\n",
      "Epoch 189/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 4.3346 - val_loss: 4.5499\n",
      "Epoch 190/200\n",
      "251/251 [==============================] - 0s 32us/sample - loss: 4.3467 - val_loss: 6.0171\n",
      "Epoch 191/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 4.9909 - val_loss: 4.3492\n",
      "Epoch 192/200\n",
      "251/251 [==============================] - 0s 30us/sample - loss: 4.6595 - val_loss: 3.7772\n",
      "Epoch 193/200\n",
      "251/251 [==============================] - 0s 36us/sample - loss: 4.7210 - val_loss: 4.4339\n",
      "Epoch 194/200\n",
      "251/251 [==============================] - 0s 31us/sample - loss: 4.3899 - val_loss: 4.9792\n",
      "Epoch 195/200\n",
      "251/251 [==============================] - 0s 40us/sample - loss: 4.5814 - val_loss: 4.4420\n",
      "Epoch 196/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 5.0369 - val_loss: 4.1344\n",
      "Epoch 197/200\n",
      "251/251 [==============================] - 0s 38us/sample - loss: 4.9027 - val_loss: 4.7164\n",
      "Epoch 198/200\n",
      "251/251 [==============================] - 0s 42us/sample - loss: 4.1891 - val_loss: 4.0949\n",
      "Epoch 199/200\n",
      "251/251 [==============================] - 0s 35us/sample - loss: 4.5692 - val_loss: 4.1926\n",
      "Epoch 200/200\n",
      "251/251 [==============================] - 0s 37us/sample - loss: 4.0574 - val_loss: 4.3664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a35bb5390>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeld2.fit(\n",
    "  normed_train_data, train_labels,\n",
    "  epochs=200, validation_split = 0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 251 samples, validate on 63 samples\n",
      "Epoch 1/50\n",
      "251/251 [==============================] - 0s 2ms/sample - loss: 570.5004 - mean_squared_error: 575.1005 - val_loss: 525.9553 - val_mean_squared_error: 553.2946\n",
      "Epoch 2/50\n",
      "251/251 [==============================] - 0s 81us/sample - loss: 453.3552 - mean_squared_error: 504.9183 - val_loss: 380.3773 - val_mean_squared_error: 456.9494\n",
      "Epoch 3/50\n",
      "251/251 [==============================] - 0s 93us/sample - loss: 322.3512 - mean_squared_error: 407.9053 - val_loss: 235.8625 - val_mean_squared_error: 325.7866\n",
      "Epoch 4/50\n",
      "251/251 [==============================] - 0s 63us/sample - loss: 196.5197 - mean_squared_error: 286.2785 - val_loss: 126.2707 - val_mean_squared_error: 203.3328\n",
      "Epoch 5/50\n",
      "251/251 [==============================] - 0s 60us/sample - loss: 108.7934 - mean_squared_error: 173.7511 - val_loss: 61.6762 - val_mean_squared_error: 110.7957\n",
      "Epoch 6/50\n",
      "251/251 [==============================] - 0s 59us/sample - loss: 52.4355 - mean_squared_error: 94.1882 - val_loss: 31.7167 - val_mean_squared_error: 61.2742\n",
      "Epoch 7/50\n",
      "251/251 [==============================] - 0s 56us/sample - loss: 28.2033 - mean_squared_error: 57.0412 - val_loss: 20.1608 - val_mean_squared_error: 43.6417\n",
      "Epoch 8/50\n",
      "251/251 [==============================] - 0s 54us/sample - loss: 21.2526 - mean_squared_error: 49.5770 - val_loss: 14.4097 - val_mean_squared_error: 38.2311\n",
      "Epoch 9/50\n",
      "251/251 [==============================] - 0s 54us/sample - loss: 15.6552 - mean_squared_error: 40.5641 - val_loss: 10.7044 - val_mean_squared_error: 36.0982\n",
      "Epoch 10/50\n",
      "251/251 [==============================] - 0s 48us/sample - loss: 11.0615 - mean_squared_error: 33.0170 - val_loss: 7.9234 - val_mean_squared_error: 26.6053\n",
      "Epoch 11/50\n",
      "251/251 [==============================] - 0s 51us/sample - loss: 9.0254 - mean_squared_error: 30.6298 - val_loss: 6.3119 - val_mean_squared_error: 22.6583\n",
      "Epoch 12/50\n",
      "251/251 [==============================] - 0s 52us/sample - loss: 6.8391 - mean_squared_error: 23.0185 - val_loss: 5.0759 - val_mean_squared_error: 19.1708\n",
      "Epoch 13/50\n",
      "251/251 [==============================] - 0s 54us/sample - loss: 6.6786 - mean_squared_error: 27.6804 - val_loss: 4.2443 - val_mean_squared_error: 19.8969\n",
      "Epoch 14/50\n",
      "251/251 [==============================] - 0s 55us/sample - loss: 5.3503 - mean_squared_error: 23.8284 - val_loss: 3.7111 - val_mean_squared_error: 19.1944\n",
      "Epoch 15/50\n",
      "251/251 [==============================] - 0s 53us/sample - loss: 5.3586 - mean_squared_error: 27.2697 - val_loss: 3.5354 - val_mean_squared_error: 16.3659\n",
      "Epoch 16/50\n",
      "251/251 [==============================] - 0s 46us/sample - loss: 4.7191 - mean_squared_error: 25.3211 - val_loss: 2.9893 - val_mean_squared_error: 17.3314\n",
      "Epoch 17/50\n",
      "251/251 [==============================] - 0s 48us/sample - loss: 3.9690 - mean_squared_error: 26.8141 - val_loss: 2.7624 - val_mean_squared_error: 15.5003\n",
      "Epoch 18/50\n",
      "251/251 [==============================] - 0s 50us/sample - loss: 3.8338 - mean_squared_error: 27.1539 - val_loss: 2.6382 - val_mean_squared_error: 12.5234\n",
      "Epoch 19/50\n",
      "251/251 [==============================] - 0s 55us/sample - loss: 3.5114 - mean_squared_error: 22.5832 - val_loss: 2.5601 - val_mean_squared_error: 19.3776\n",
      "Epoch 20/50\n",
      "251/251 [==============================] - 0s 50us/sample - loss: 3.6019 - mean_squared_error: 28.5433 - val_loss: 2.5002 - val_mean_squared_error: 17.6079\n",
      "Epoch 21/50\n",
      "251/251 [==============================] - 0s 52us/sample - loss: 3.1597 - mean_squared_error: 24.7704 - val_loss: 2.4877 - val_mean_squared_error: 14.5293\n",
      "Epoch 22/50\n",
      "251/251 [==============================] - 0s 49us/sample - loss: 2.8495 - mean_squared_error: 25.6793 - val_loss: 2.4719 - val_mean_squared_error: 20.9100\n",
      "Epoch 23/50\n",
      "251/251 [==============================] - 0s 46us/sample - loss: 2.9539 - mean_squared_error: 25.0216 - val_loss: 2.4176 - val_mean_squared_error: 16.1486\n",
      "Epoch 24/50\n",
      "251/251 [==============================] - 0s 47us/sample - loss: 3.0780 - mean_squared_error: 28.7958 - val_loss: 2.4641 - val_mean_squared_error: 24.3615\n",
      "Epoch 25/50\n",
      "251/251 [==============================] - 0s 51us/sample - loss: 2.7792 - mean_squared_error: 28.8582 - val_loss: 2.4331 - val_mean_squared_error: 21.0215\n",
      "Epoch 26/50\n",
      "251/251 [==============================] - 0s 51us/sample - loss: 2.9006 - mean_squared_error: 28.6124 - val_loss: 2.4819 - val_mean_squared_error: 17.7852\n",
      "Epoch 27/50\n",
      "251/251 [==============================] - 0s 50us/sample - loss: 2.7178 - mean_squared_error: 25.3203 - val_loss: 2.4817 - val_mean_squared_error: 23.7804\n",
      "Epoch 28/50\n",
      "251/251 [==============================] - 0s 48us/sample - loss: 2.7590 - mean_squared_error: 35.3984 - val_loss: 2.4768 - val_mean_squared_error: 21.9696\n",
      "Epoch 29/50\n",
      "251/251 [==============================] - 0s 47us/sample - loss: 2.8529 - mean_squared_error: 36.7761 - val_loss: 2.4671 - val_mean_squared_error: 24.0349\n",
      "Epoch 30/50\n",
      "251/251 [==============================] - 0s 50us/sample - loss: 2.8065 - mean_squared_error: 35.6293 - val_loss: 2.4851 - val_mean_squared_error: 22.6193\n",
      "Epoch 31/50\n",
      "251/251 [==============================] - 0s 55us/sample - loss: 2.6834 - mean_squared_error: 26.1986 - val_loss: 2.4816 - val_mean_squared_error: 26.3121\n",
      "Epoch 32/50\n",
      "251/251 [==============================] - 0s 51us/sample - loss: 2.7517 - mean_squared_error: 29.4073 - val_loss: 2.5497 - val_mean_squared_error: 47.4354\n",
      "Epoch 33/50\n",
      "251/251 [==============================] - 0s 46us/sample - loss: 2.7119 - mean_squared_error: 29.8551 - val_loss: 2.5525 - val_mean_squared_error: 26.9115\n",
      "Epoch 34/50\n",
      "251/251 [==============================] - 0s 48us/sample - loss: 2.6864 - mean_squared_error: 33.0320 - val_loss: 2.4955 - val_mean_squared_error: 22.4024\n",
      "Epoch 35/50\n",
      "251/251 [==============================] - 0s 47us/sample - loss: 2.7285 - mean_squared_error: 35.4304 - val_loss: 2.5612 - val_mean_squared_error: 27.4101\n",
      "Epoch 36/50\n",
      "251/251 [==============================] - 0s 51us/sample - loss: 2.6930 - mean_squared_error: 22.0361 - val_loss: 2.5119 - val_mean_squared_error: 26.4139\n",
      "Epoch 37/50\n",
      "251/251 [==============================] - 0s 50us/sample - loss: 2.7854 - mean_squared_error: 31.8888 - val_loss: 2.5044 - val_mean_squared_error: 20.9010\n",
      "Epoch 38/50\n",
      "251/251 [==============================] - 0s 49us/sample - loss: 2.7147 - mean_squared_error: 26.5266 - val_loss: 2.4984 - val_mean_squared_error: 17.4315\n",
      "Epoch 39/50\n",
      "251/251 [==============================] - 0s 47us/sample - loss: 2.7028 - mean_squared_error: 36.8199 - val_loss: 2.4972 - val_mean_squared_error: 21.0431\n",
      "Epoch 40/50\n",
      "251/251 [==============================] - 0s 48us/sample - loss: 2.7198 - mean_squared_error: 38.9480 - val_loss: 2.4866 - val_mean_squared_error: 26.7560\n",
      "Epoch 41/50\n",
      "251/251 [==============================] - 0s 51us/sample - loss: 2.6398 - mean_squared_error: 30.4947 - val_loss: 2.5070 - val_mean_squared_error: 22.6130\n",
      "Epoch 42/50\n",
      "251/251 [==============================] - 0s 51us/sample - loss: 2.7755 - mean_squared_error: 34.3943 - val_loss: 2.5005 - val_mean_squared_error: 35.4587\n",
      "Epoch 43/50\n",
      "251/251 [==============================] - 0s 50us/sample - loss: 2.6813 - mean_squared_error: 30.8726 - val_loss: 2.5014 - val_mean_squared_error: 16.7568\n",
      "Epoch 44/50\n",
      "251/251 [==============================] - 0s 51us/sample - loss: 2.7138 - mean_squared_error: 30.8207 - val_loss: 2.4945 - val_mean_squared_error: 17.0099\n",
      "Epoch 45/50\n",
      "251/251 [==============================] - 0s 56us/sample - loss: 2.6516 - mean_squared_error: 25.1779 - val_loss: 2.5071 - val_mean_squared_error: 22.0255\n",
      "Epoch 46/50\n",
      "251/251 [==============================] - 0s 57us/sample - loss: 2.6592 - mean_squared_error: 33.2297 - val_loss: 2.4836 - val_mean_squared_error: 23.8184\n",
      "Epoch 47/50\n",
      "251/251 [==============================] - 0s 57us/sample - loss: 2.6263 - mean_squared_error: 23.4419 - val_loss: 2.4876 - val_mean_squared_error: 25.3780\n",
      "Epoch 48/50\n",
      "251/251 [==============================] - 0s 58us/sample - loss: 2.6970 - mean_squared_error: 31.1125 - val_loss: 2.4802 - val_mean_squared_error: 24.1951\n",
      "Epoch 49/50\n",
      "251/251 [==============================] - 0s 54us/sample - loss: 2.6549 - mean_squared_error: 29.5559 - val_loss: 2.4655 - val_mean_squared_error: 22.6603\n",
      "Epoch 50/50\n",
      "251/251 [==============================] - 0s 54us/sample - loss: 2.6181 - mean_squared_error: 29.9147 - val_loss: 2.4652 - val_mean_squared_error: 37.2320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a524a1cf8>"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeld.fit(\n",
    "  normed_train_data, train_labels,\n",
    "  epochs=50, validation_split = 0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeld3=tf.keras.Model(inputs, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model3 goes from the inputs to the mean and variance of normal distribution. The variance thus gives the aleatoric uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanvar=[]\n",
    "D=1000\n",
    "for i in range(0,D):\n",
    "    l=modeld3.predict(example_batch)\n",
    "    l[:,1]=1e-3 + tf.math.softplus(0.05 * l[:,1])\n",
    "    meanvar.append(l)\n",
    "meanss=np.mean(meanvar,axis=0) #Mean of means and variances\n",
    "varss= np.var(meanvar,axis=0)  #Variance of means and variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6009 (pid 31971), started 0:02:23 ago. (Use '!kill 31971' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6009\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a43ae7ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load TENSORBOARD\n",
    "%load_ext tensorboard\n",
    "# Start TENSORBOARD\n",
    "%tensorboard --logdir my_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeld.save(\"modeld.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Comparing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining aleatoric and epistemic uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach is as follows: to deal with aleatoric uncertainty I output the mean and variance of a normal distribution and I use the negative log likelihood as the loss function. In order to deal with epistemic uncertainty I let the network learn a distribution over the weights instead of point estimates. This means that everytime I put some datapoint through my trained network it will output a different (!) distribution. I can then do this a number of times and look at the mean of means of the distributions. I can also look at the variance of the means of distributions. This will tell me how certain my model is of the mean (epistemic uncertainty in this case). I can also look at the mean of the variances of the distribution (this tells me how certain my model is about the aleatoric uncertainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Stuf that is now unneccesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def epi(x):\n",
    "    B1=4\n",
    "    B=4\n",
    "    de=modeld(x)\n",
    "    d=de.sample()\n",
    "    for i in range(1,B):\n",
    "            l=modeld(x).sample()\n",
    "            d=np.hstack((d,l))      \n",
    "    return(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def epi2(x):\n",
    "    B1=50 #The number of distributions I evaluate \n",
    "    B2=1000 #The number of points I take from a distribution to determine the variance\n",
    "    with keras.backend.learning_phase_scope(1):\n",
    "        de=modeld(x)\n",
    "        d=de.sample()\n",
    "        for i in range(1,B2):\n",
    "            l=de.sample()\n",
    "            d=np.hstack((d,l))\n",
    "        e=np.var(d,axis=1)\n",
    "        for b in range(0,B1):\n",
    "            de=modeld(x)\n",
    "            d=de.sample()\n",
    "            for c in range(1,B2):\n",
    "                l=de.sample()\n",
    "                d=np.hstack((d,l))\n",
    "            t=np.var(d,axis=1)\n",
    "            e=np.hstack((e,t))\n",
    "        final=e.reshape(len(x),np.int(len(e)/len(x)))\n",
    "    return(np.sqrt(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def meane(x):\n",
    "    B=100\n",
    "    d=[]\n",
    "    with keras.backend.learning_phase_scope(1):\n",
    "        a=modeld(x).mean()\n",
    "        for i in range(1,B):\n",
    "            l=modeld(x).mean()\n",
    "            a=np.hstack((a,l))\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epis=epi2(example_batch.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "means=meane(example_batch.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained a neural network that outputs a distribution. This distribution has a mean and a variance. Since the weights are not a point estimate but a distribution, I get a different distribution every time I put data trough the network! I did this a number of times in order to get the average of the means of the distribution and the variance of the variances of the distributions. This second quantity can be thought of as the model uncertainty of the aleatoric uncertainty! The first column gives the mean (over all the distribution realisations) of the mean of the distributions. The second column gives the variance of the means (how much do the means change if we pass the same point through the network a number of times?). The third column is the mean of the variances of the distributions. I calculated this by taking B1 different distributions and sampling B2 points from each distribution to get the variance of each distribution. Then I looked at the means of those variances. The variance of the variances is given in column 4 and the true value in column 6. Note that I used a dropout method to get the epistemic uncertainties. This gives uncalibrated standard deviations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected value</th>\n",
       "      <th>epistemic uncertainty</th>\n",
       "      <th>aleatoric uncertainty</th>\n",
       "      <th>epst uncrtnty of altrc uncrtnty</th>\n",
       "      <th>true value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.599900</td>\n",
       "      <td>5.850087</td>\n",
       "      <td>1.858276</td>\n",
       "      <td>0.577967</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.514923</td>\n",
       "      <td>3.581459</td>\n",
       "      <td>1.755138</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.265011</td>\n",
       "      <td>3.411803</td>\n",
       "      <td>1.601677</td>\n",
       "      <td>0.361993</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.880915</td>\n",
       "      <td>4.479910</td>\n",
       "      <td>2.355287</td>\n",
       "      <td>0.428901</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.890968</td>\n",
       "      <td>3.425918</td>\n",
       "      <td>1.928373</td>\n",
       "      <td>0.333183</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.295409</td>\n",
       "      <td>3.923345</td>\n",
       "      <td>1.897949</td>\n",
       "      <td>0.452535</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.035235</td>\n",
       "      <td>3.658480</td>\n",
       "      <td>1.814502</td>\n",
       "      <td>0.407024</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.109076</td>\n",
       "      <td>3.800194</td>\n",
       "      <td>1.890169</td>\n",
       "      <td>0.452534</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.065075</td>\n",
       "      <td>3.340594</td>\n",
       "      <td>1.874552</td>\n",
       "      <td>0.328103</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30.656433</td>\n",
       "      <td>5.851227</td>\n",
       "      <td>2.966657</td>\n",
       "      <td>0.623476</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25.620476</td>\n",
       "      <td>3.904865</td>\n",
       "      <td>2.506334</td>\n",
       "      <td>0.385753</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26.710224</td>\n",
       "      <td>4.792919</td>\n",
       "      <td>2.532451</td>\n",
       "      <td>0.470304</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.907470</td>\n",
       "      <td>3.537383</td>\n",
       "      <td>1.825448</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25.430149</td>\n",
       "      <td>3.915085</td>\n",
       "      <td>2.470952</td>\n",
       "      <td>0.377252</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.106276</td>\n",
       "      <td>3.751588</td>\n",
       "      <td>1.811251</td>\n",
       "      <td>0.408300</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.675822</td>\n",
       "      <td>3.503765</td>\n",
       "      <td>1.802748</td>\n",
       "      <td>0.379927</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.631678</td>\n",
       "      <td>3.665877</td>\n",
       "      <td>1.882138</td>\n",
       "      <td>0.401002</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15.580611</td>\n",
       "      <td>3.642375</td>\n",
       "      <td>1.852881</td>\n",
       "      <td>0.410254</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.954979</td>\n",
       "      <td>3.274133</td>\n",
       "      <td>1.942779</td>\n",
       "      <td>0.317120</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.125637</td>\n",
       "      <td>3.645149</td>\n",
       "      <td>1.837786</td>\n",
       "      <td>0.408975</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15.823359</td>\n",
       "      <td>4.051943</td>\n",
       "      <td>1.952373</td>\n",
       "      <td>0.463563</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27.400873</td>\n",
       "      <td>4.731032</td>\n",
       "      <td>2.588003</td>\n",
       "      <td>0.493772</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21.249929</td>\n",
       "      <td>4.031857</td>\n",
       "      <td>2.012869</td>\n",
       "      <td>0.368821</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.779392</td>\n",
       "      <td>3.910447</td>\n",
       "      <td>2.044353</td>\n",
       "      <td>0.366595</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31.405563</td>\n",
       "      <td>5.717769</td>\n",
       "      <td>3.017626</td>\n",
       "      <td>0.601694</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.569090</td>\n",
       "      <td>3.791254</td>\n",
       "      <td>2.484051</td>\n",
       "      <td>0.360674</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.582443</td>\n",
       "      <td>3.174006</td>\n",
       "      <td>1.764236</td>\n",
       "      <td>0.311655</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25.674255</td>\n",
       "      <td>4.186599</td>\n",
       "      <td>2.405821</td>\n",
       "      <td>0.416193</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>17.331524</td>\n",
       "      <td>3.121821</td>\n",
       "      <td>1.804522</td>\n",
       "      <td>0.303401</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19.228764</td>\n",
       "      <td>3.763631</td>\n",
       "      <td>1.984768</td>\n",
       "      <td>0.393622</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30.236031</td>\n",
       "      <td>4.948812</td>\n",
       "      <td>2.853492</td>\n",
       "      <td>0.536624</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>21.543369</td>\n",
       "      <td>3.381965</td>\n",
       "      <td>2.104471</td>\n",
       "      <td>0.313858</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>19.864885</td>\n",
       "      <td>3.164146</td>\n",
       "      <td>1.999152</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>23.183676</td>\n",
       "      <td>4.195824</td>\n",
       "      <td>2.149730</td>\n",
       "      <td>0.400098</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>28.038931</td>\n",
       "      <td>4.019726</td>\n",
       "      <td>2.694366</td>\n",
       "      <td>0.379020</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>17.037258</td>\n",
       "      <td>3.747010</td>\n",
       "      <td>1.883601</td>\n",
       "      <td>0.407462</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20.740364</td>\n",
       "      <td>3.100296</td>\n",
       "      <td>2.064529</td>\n",
       "      <td>0.299701</td>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>31.371817</td>\n",
       "      <td>5.174504</td>\n",
       "      <td>3.084528</td>\n",
       "      <td>0.530417</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>31.064449</td>\n",
       "      <td>5.537411</td>\n",
       "      <td>2.905294</td>\n",
       "      <td>0.583813</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>18.461674</td>\n",
       "      <td>3.004905</td>\n",
       "      <td>1.879257</td>\n",
       "      <td>0.290153</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>32.201145</td>\n",
       "      <td>5.590286</td>\n",
       "      <td>3.045620</td>\n",
       "      <td>0.595623</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>30.967783</td>\n",
       "      <td>5.009436</td>\n",
       "      <td>2.927569</td>\n",
       "      <td>0.520647</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>17.556465</td>\n",
       "      <td>4.071537</td>\n",
       "      <td>1.957695</td>\n",
       "      <td>0.441886</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>35.846539</td>\n",
       "      <td>6.910445</td>\n",
       "      <td>3.465062</td>\n",
       "      <td>0.750179</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32.744518</td>\n",
       "      <td>4.841694</td>\n",
       "      <td>3.085187</td>\n",
       "      <td>0.473620</td>\n",
       "      <td>36.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>35.824600</td>\n",
       "      <td>6.512582</td>\n",
       "      <td>3.426525</td>\n",
       "      <td>0.698535</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>22.325430</td>\n",
       "      <td>3.347651</td>\n",
       "      <td>2.190786</td>\n",
       "      <td>0.316424</td>\n",
       "      <td>20.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23.661768</td>\n",
       "      <td>3.352043</td>\n",
       "      <td>2.287962</td>\n",
       "      <td>0.316162</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>19.854286</td>\n",
       "      <td>4.004655</td>\n",
       "      <td>2.037249</td>\n",
       "      <td>0.413686</td>\n",
       "      <td>18.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>28.111458</td>\n",
       "      <td>4.559640</td>\n",
       "      <td>2.597651</td>\n",
       "      <td>0.442398</td>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    expected value  epistemic uncertainty  aleatoric uncertainty  \\\n",
       "0        12.599900               5.850087               1.858276   \n",
       "1        13.514923               3.581459               1.755138   \n",
       "2        12.265011               3.411803               1.601677   \n",
       "3        24.880915               4.479910               2.355287   \n",
       "4        18.890968               3.425918               1.928373   \n",
       "5        15.295409               3.923345               1.897949   \n",
       "6        15.035235               3.658480               1.814502   \n",
       "7        15.109076               3.800194               1.890169   \n",
       "8        18.065075               3.340594               1.874552   \n",
       "9        30.656433               5.851227               2.966657   \n",
       "10       25.620476               3.904865               2.506334   \n",
       "11       26.710224               4.792919               2.532451   \n",
       "12       15.907470               3.537383               1.825448   \n",
       "13       25.430149               3.915085               2.470952   \n",
       "14       15.106276               3.751588               1.811251   \n",
       "15       15.675822               3.503765               1.802748   \n",
       "16       16.631678               3.665877               1.882138   \n",
       "17       15.580611               3.642375               1.852881   \n",
       "18       18.954979               3.274133               1.942779   \n",
       "19       15.125637               3.645149               1.837786   \n",
       "20       15.823359               4.051943               1.952373   \n",
       "21       27.400873               4.731032               2.588003   \n",
       "22       21.249929               4.031857               2.012869   \n",
       "23       21.779392               3.910447               2.044353   \n",
       "24       31.405563               5.717769               3.017626   \n",
       "25       25.569090               3.791254               2.484051   \n",
       "26       16.582443               3.174006               1.764236   \n",
       "27       25.674255               4.186599               2.405821   \n",
       "28       17.331524               3.121821               1.804522   \n",
       "29       19.228764               3.763631               1.984768   \n",
       "30       30.236031               4.948812               2.853492   \n",
       "31       21.543369               3.381965               2.104471   \n",
       "32       19.864885               3.164146               1.999152   \n",
       "33       23.183676               4.195824               2.149730   \n",
       "34       28.038931               4.019726               2.694366   \n",
       "35       17.037258               3.747010               1.883601   \n",
       "36       20.740364               3.100296               2.064529   \n",
       "37       31.371817               5.174504               3.084528   \n",
       "38       31.064449               5.537411               2.905294   \n",
       "39       18.461674               3.004905               1.879257   \n",
       "40       32.201145               5.590286               3.045620   \n",
       "41       30.967783               5.009436               2.927569   \n",
       "42       17.556465               4.071537               1.957695   \n",
       "43       35.846539               6.910445               3.465062   \n",
       "44       32.744518               4.841694               3.085187   \n",
       "45       35.824600               6.512582               3.426525   \n",
       "46       22.325430               3.347651               2.190786   \n",
       "47       23.661768               3.352043               2.287962   \n",
       "48       19.854286               4.004655               2.037249   \n",
       "49       28.111458               4.559640               2.597651   \n",
       "\n",
       "    epst uncrtnty of altrc uncrtnty  true value  \n",
       "0                          0.577967        15.0  \n",
       "1                          0.392781        10.0  \n",
       "2                          0.361993         9.0  \n",
       "3                          0.428901        25.0  \n",
       "4                          0.333183        19.0  \n",
       "5                          0.452535        14.0  \n",
       "6                          0.407024        14.0  \n",
       "7                          0.452534        13.0  \n",
       "8                          0.328103        18.0  \n",
       "9                          0.623476        35.0  \n",
       "10                         0.385753        25.0  \n",
       "11                         0.470304        19.0  \n",
       "12                         0.392913        13.0  \n",
       "13                         0.377252        28.0  \n",
       "14                         0.408300        13.0  \n",
       "15                         0.379927        14.0  \n",
       "16                         0.401002        15.0  \n",
       "17                         0.410254        13.0  \n",
       "18                         0.317120        18.0  \n",
       "19                         0.408975        12.0  \n",
       "20                         0.463563        16.0  \n",
       "21                         0.493772        24.0  \n",
       "22                         0.368821        19.0  \n",
       "23                         0.366595        24.0  \n",
       "24                         0.601694        31.0  \n",
       "25                         0.360674        26.0  \n",
       "26                         0.311655        16.0  \n",
       "27                         0.416193        24.0  \n",
       "28                         0.303401        18.0  \n",
       "29                         0.393622        20.0  \n",
       "30                         0.536624        29.0  \n",
       "31                         0.313858        18.0  \n",
       "32                         0.311111        19.0  \n",
       "33                         0.400098        22.0  \n",
       "34                         0.379020        26.0  \n",
       "35                         0.407462        17.5  \n",
       "36                         0.299701        22.5  \n",
       "37                         0.530417        29.0  \n",
       "38                         0.583813        29.0  \n",
       "39                         0.290153        20.0  \n",
       "40                         0.595623        32.0  \n",
       "41                         0.520647        28.0  \n",
       "42                         0.441886        13.0  \n",
       "43                         0.750179        43.1  \n",
       "44                         0.473620        36.1  \n",
       "45                         0.698535        32.8  \n",
       "46                         0.316424        20.5  \n",
       "47                         0.316162        20.8  \n",
       "48                         0.413686        18.1  \n",
       "49                         0.442398        27.5  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(data=np.column_stack(([a[0] for a in np.mean(meanvar,axis=0)],\n",
    "                                         (np.sqrt([a[0] for a in np.var(meanvar,axis=0)])),\n",
    "                                         ([a[1] for a in np.mean((meanvar),axis=0)]),\n",
    "                                         np.sqrt([a[1] for a in np.var((meanvar),axis=0)]),\n",
    "                                         test_labels[:tn])),\n",
    "                  columns=['expected value',\n",
    "                           'epistemic uncertainty',\n",
    "                           'aleatoric uncertainty',\n",
    "                           'epst uncrtnty of altrc uncrtnty',\n",
    "                           'true value'])\n",
    "df2\n",
    "#N.B. I messed up the first data point on purpose!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_batch.loc[9]=[3,3,8,3,3,3,0.8,-0.46,-0.46] #This allows me to deliberately mess up some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0.08\n",
    "df3=(df2[2:50]['expected value']-df2[2:50]['true value'])/((((df2[2:50]['epistemic uncertainty']**2+df2[2:50]['aleatoric uncertainty'])**2))**0.5) #Both uncertainties estimates\n",
    "df4=(df2[2:50]['expected value']-df2[2:50]['true value'])/(df2[2:50]['aleatoric uncertainty'])**0.5 #Aleatoric uncertainty estimation\n",
    "df5=(df2[2:50]['expected value']-df2[2:50]['true value'])/(((((c*df2[2:50]['epistemic uncertainty'])**2+df2[2:50]['aleatoric uncertainty'])**2))**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I look at the (mu-muhat)/sigmahat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean= 0.05685051066121586 std= 0.11463896314492253\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df3.hist()\n",
    "print('mean=',df3.mean(),'std=',np.sqrt(df3.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean= 0.7004499981041756 std= 1.5421414027639673\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df4.hist()\n",
    "print('mean=',df4.mean(),'std=',np.sqrt(df4.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When just looking at the aleatoric uncertainty we notics that the model overestimates the uncertainty. This could be because part of the uncertainty is in fact due to the epistemic uncertainty. The epistemic uncertainty via dropout is known to not be calibrated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean= 0.47719672691255416 std= 0.956049844057355\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df5.hist()\n",
    "print('mean=',df5.mean(),'std=',np.sqrt(df5.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
